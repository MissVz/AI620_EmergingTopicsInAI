{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6b0c7d-c2d2-486a-858c-35b8efc3089e",
      "metadata": {
        "tags": [],
        "id": "4f6b0c7d-c2d2-486a-858c-35b8efc3089e",
        "outputId": "fba2de84-3e47-4bef-b54e-0a79543d4619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory:  /home/ec2-user/SageMaker/wikitext-2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def check_create_dir(dir):\n",
        "    if os.path.exists(dir):\n",
        "        shutil.rmtree(dir)\n",
        "    os.mkdir(dir)\n",
        "\n",
        "\n",
        "dataset = 'wikitext-2'\n",
        "current_dir = os.getcwd()\n",
        "data_dir = os.path.join(current_dir, dataset)\n",
        "check_create_dir(data_dir)\n",
        "os.chdir(data_dir)\n",
        "print(\"Current directory: \", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404b7120-c433-4ddd-907a-9138a3e62c81",
      "metadata": {
        "tags": [],
        "id": "404b7120-c433-4ddd-907a-9138a3e62c81",
        "outputId": "3c9277e1-2999-4d93-fa8d-9566c4aa6c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "list dir:  []\n"
          ]
        }
      ],
      "source": [
        "print(\"list dir: \", os.listdir(os.getcwd()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e999a54a-e646-4d18-9a7d-bd9a32f21fda",
      "metadata": {
        "tags": [],
        "id": "e999a54a-e646-4d18-9a7d-bd9a32f21fda"
      },
      "outputs": [],
      "source": [
        "def is_document_start(line):\n",
        "    if len(line) < 4:\n",
        "        return False\n",
        "    if line[0] == '=' and line[-1] == '=':\n",
        "        if line[2] != '=':\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69a3caa-995f-492d-a9c1-c157565126e7",
      "metadata": {
        "tags": [],
        "id": "e69a3caa-995f-492d-a9c1-c157565126e7"
      },
      "outputs": [],
      "source": [
        "def token_list_per_doc(input_dir, token_file):\n",
        "    lines_list = []\n",
        "    line_prev = ''\n",
        "    prev_line_start_doc = False\n",
        "    with open(os.path.join(input_dir, token_file), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if prev_line_start_doc and line:\n",
        "                lines_list.pop()\n",
        "                lines_list[-1] = lines_list[-1] + ' ' + line_prev\n",
        "            if line:\n",
        "                if is_document_start(line) and not line_prev:\n",
        "                    lines_list.append(line)\n",
        "                    prev_line_start_doc = True\n",
        "                else:\n",
        "                    lines_list[-1] = lines_list[-1] + \" \" + line\n",
        "                    prev_line_start_doc = False\n",
        "            else:\n",
        "                prev_line_start_doc = False\n",
        "            line_prev = line\n",
        "    print(\"{} documents parsed!\".format(len(lines_list)))\n",
        "    return lines_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f4bd06-3384-4e55-951d-8c8598ea1566",
      "metadata": {
        "id": "24f4bd06-3384-4e55-951d-8c8598ea1566",
        "outputId": "b1326d7b-1541-410f-bbc1-1269cf560428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "600 documents parsed!\n",
            "60 documents parsed!\n",
            "60 documents parsed!\n"
          ]
        }
      ],
      "source": [
        "# path for the train file, validation file and test file\n",
        "train_file = 'wiki.train.tokens'\n",
        "val_file = 'wiki.valid.tokens'\n",
        "test_file = 'wiki.test.tokens'\n",
        "\n",
        "# parse documents\n",
        "train_doc_list = token_list_per_doc(data_dir, train_file)\n",
        "val_doc_list = token_list_per_doc(data_dir, val_file)\n",
        "test_doc_list = token_list_per_doc(data_dir, test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40e5cb4-dd2a-44f7-8584-9283248d2550",
      "metadata": {
        "tags": [],
        "id": "e40e5cb4-dd2a-44f7-8584-9283248d2550",
        "outputId": "e50cccb5-de80-4dff-b36e-210ce7c1f638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e71d2c-509a-44e8-baba-9bb454da54ae",
      "metadata": {
        "tags": [],
        "id": "b5e71d2c-509a-44e8-baba-9bb454da54ae",
        "outputId": "736491a5-6f87-4205-f297-e55563be3c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Take a quick search on nltk. What does it do?\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "print(\"done\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "token_pattern = re.compile(r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "\n",
        "class LemmaTokenier(object):\n",
        "    def __init__(self):\n",
        "        # examples: https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet\n",
        "        self.wnl = WordNetLEmmatizer\n",
        "\n",
        "    def _call__(self, doc):\n",
        "        return [\n",
        "            self.unl.lemmatie(t)\n",
        "            for t in doc.split()\n",
        "            if len(t) >= 2 and re.match(\"[a-z].*\", t) and re.match(tocken_pattern, t)\n",
        "                                        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1f21dd-1f3c-405e-ad9d-98a8663b53d6",
      "metadata": {
        "tags": [],
        "id": "be1f21dd-1f3c-405e-ad9d-98a8663b53d6",
        "outputId": "21cf1a43-0a4f-4204-d729-df07ffbc991f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatizing and counting, this may take a few minutes...\n",
            "vocab size 20439\n",
            "Done. Time elapsed:: 2.0s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "print(\"Lemmatizing and counting, this may take a few minutes...\")\n",
        "\n",
        "start_time = time.time()\n",
        "# https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "vectorizer = CountVectorizer(\n",
        "    input = 'content',\n",
        "    analyzer = 'word',\n",
        "    stop_words = 'english',\n",
        "    #tokenixer = LemmaTokenizer(),\n",
        "    max_df = 0.9,\n",
        "    min_df = 3,\n",
        ")\n",
        "\n",
        "train_vectors = vectorizer.fit_transform(train_doc_list)\n",
        "val_vectors = vectorizer.transform(val_doc_list)\n",
        "test_vectors = vectorizer.transform(test_doc_list)\n",
        "\n",
        "vocab_list = vectorizer.get_feature_names_out()\n",
        "vocab_size = len(vocab_list)\n",
        "print('vocab size', vocab_size)\n",
        "print('Done. Time elapsed:: {:.2}s'.format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c7e7a7-4715-4390-bd89-3e8bfeffcfa5",
      "metadata": {
        "tags": [],
        "id": "b3c7e7a7-4715-4390-bd89-3e8bfeffcfa5",
        "outputId": "9a9b2585-f991-4100-fe10-9807d17db082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'> float32\n",
            "<class 'scipy.sparse._csr.csr_matrix'> float32\n",
            "<class 'scipy.sparse._csr.csr_matrix'> float32\n"
          ]
        }
      ],
      "source": [
        "import scipy.sparse as sparse\n",
        "\n",
        "def shuffle_and_dtype(vectors):\n",
        "    # takes a 2D array vectors as input and performs two main operations.\n",
        "    # First, it shuffles the rows of the array randomly by creating a shuffled index and reordering the rows accordingly.\n",
        "    # Then, it converts the shuffled array into a sparse matrix of type csr_matrix (Compressed Sparse Row format) with a data type of float32\n",
        "    # Finally it prints its type and data type, and returns the result.\n",
        "\n",
        "\n",
        "train_vectors = shuffle_and_dtype(train_vectors)\n",
        "val_vectors = shuffle_and_dtype(val_vectors)\n",
        "test_vectors = shuffle_and_dtype(test_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828dd19b-83ce-44cb-b382-8e6b08a10e80",
      "metadata": {
        "tags": [],
        "id": "828dd19b-83ce-44cb-b382-8e6b08a10e80",
        "outputId": "8ba339cd-5f3e-49dc-d628-409d378f0515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
            "Saved data to /home/ec2-user/SageMaker/wikitext-2/train/train_part0.pdr\n",
            "Saved data to /home/ec2-user/SageMaker/wikitext-2/train/train_part1.pdr\n",
            "Saved data to /home/ec2-user/SageMaker/wikitext-2/train/train_part2.pdr\n",
            "Saved data to /home/ec2-user/SageMaker/wikitext-2/train/train_part3.pdr\n",
            "Saved data to /home/ec2-user/SageMaker/wikitext-2/Validation/val_part0.pdr\n",
            "Saved data to /home/ec2-user/SageMaker/wikitext-2/Test/test_part0.pdr\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import sagemaker.amazon.common as smac\n",
        "\n",
        "def split_convert(sparray, prefix, fname_template=\"data_part{}.pbr\", n_parts=2):\n",
        "    # The function divides a sparse array (sparray) into n_parts equal-sized chunks, with the last chunk adjusted to include any remaining rows if the division isnâ€™t exact.\n",
        "    #    Conversion to Sparse Tensor: For each chunk, it converts the data from sparse matrix format into a sparse tensor format using the smac.write_spmatrix_to_sparse_tensor method, storing it in an in-memory buffer (buf).\n",
        "    #    Saving to Disk: The buffer's content is then written to a file with a name based on the fname_template and stored in the directory specified by prefix.\n",
        "    #    File Naming and Structure: The file names follow a consistent template (data_part{}.pbr), where {} is replaced by the current part number (e.g., data_part0.pbr, data_part1.pbr, etc.).\n",
        "    #    Logging Progress: After saving each part, the function prints a confirmation message indicating where the data has been saved.\n",
        "    #    This function is designed to process large sparse arrays by splitting them and saving them in a format optimized for downstream use.\n",
        "    chunk_size = sparray.shape[0] // n_parts\n",
        "    for i in range(n_parts):\n",
        "        start = i * chunk_size\n",
        "        end = (i + 1) * chunk_size\n",
        "        if i + 1 == n_parts:\n",
        "            end = sparray.shape[0]\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
        "        buf.seek(0)\n",
        "\n",
        "        fname = os.path.join(prefix, fname_template.format(i))\n",
        "        with open(fname, 'wb') as f:\n",
        "            f.write(buf.getvalue())\n",
        "        print(\"Saved data to {}\".format(fname))\n",
        "\n",
        "\n",
        "train_data_dir = os.path.join(data_dir, 'train')\n",
        "val_data_dir = os.path.join(data_dir, 'Validation')\n",
        "test_data_dir = os.path.join(data_dir, 'Test')\n",
        "\n",
        "check_create_dir(train_data_dir)\n",
        "check_create_dir(val_data_dir)\n",
        "check_create_dir(test_data_dir)\n",
        "\n",
        "split_convert(train_vectors, prefix=train_data_dir, fname_template= \"train_part{}.pdr\", n_parts=4)\n",
        "split_convert(val_vectors, prefix=val_data_dir, fname_template= \"val_part{}.pdr\", n_parts=1)\n",
        "split_convert(test_vectors, prefix=test_data_dir, fname_template= \"test_part{}.pdr\", n_parts=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6d003c-4e99-4506-b401-22133d4c9cab",
      "metadata": {
        "tags": [],
        "id": "1f6d003c-4e99-4506-b401-22133d4c9cab"
      },
      "outputs": [],
      "source": [
        "aux_data_dir = os.path.join(data_dir, \"auxiliary\")\n",
        "check_create_dir(aux_data_dir)\n",
        "with open(os.path.join(aux_data_dir, 'vocab.txt'), \"w\", encoding=\"utf-8\") as f:\n",
        "          for item in vocab_list:\n",
        "              f.write(item + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf72b20e-3b55-4feb-8b3c-8e7496186fae",
      "metadata": {
        "tags": [],
        "id": "bf72b20e-3b55-4feb-8b3c-8e7496186fae",
        "outputId": "344b8541-8a02-435a-adeb-8fdd82c50c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set locations s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/train\n",
            "Validation set locations s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/val\n",
            "Auxiliary set locations s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/auxiliary\n",
            "Test set locations s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/test\n",
            "Trained set locations s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/output\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sagemaker\n",
        "\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "bucket = sagemaker.Session().default_bucket()\n",
        "prefix = \"ntm/\" + dataset\n",
        "\n",
        "train_prefix = os.path.join(prefix, \"train\")\n",
        "val_prefix = os.path.join(prefix, \"val\")\n",
        "aux_prefix = os.path.join(prefix, \"auxiliary\")\n",
        "test_prefix = os.path.join(prefix, 'test')\n",
        "output_prefix = os.path.join(prefix, \"output\")\n",
        "\n",
        "s3_train_data = os.path.join(\"s3://\", bucket, train_prefix)\n",
        "s3_val_data = os.path.join(\"s3://\", bucket, val_prefix)\n",
        "s3_aux_data = os.path.join(\"s3://\", bucket, aux_prefix)\n",
        "s3_test_data = os.path.join(\"s3://\", bucket, test_prefix)\n",
        "output_path= os.path.join(\"s3://\", bucket, output_prefix)\n",
        "\n",
        "print('Training set locations', s3_train_data)\n",
        "print('Validation set locations', s3_val_data)\n",
        "print('Auxiliary set locations', s3_aux_data)\n",
        "print('Test set locations', s3_test_data)\n",
        "print('Trained set locations', output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08284897-8c42-4cc6-95c4-eb225d9b4442",
      "metadata": {
        "tags": [],
        "id": "08284897-8c42-4cc6-95c4-eb225d9b4442",
        "outputId": "2730dbec-bab6-4a9a-ec7a-c207c895705d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(b'Completed 256.0 KiB/2.3 MiB (1.8 MiB/s) with 4 file(s) remaining\\rCompleted 512.0 KiB/2.3 MiB (3.5 MiB/s) with 4 file(s) remaining\\rCompleted 768.0 KiB/2.3 MiB (5.2 MiB/s) with 4 file(s) remaining\\rCompleted 1.0 MiB/2.3 MiB (6.8 MiB/s) with 4 file(s) remaining  \\rCompleted 1.2 MiB/2.3 MiB (8.5 MiB/s) with 4 file(s) remaining  \\rCompleted 1.5 MiB/2.3 MiB (10.1 MiB/s) with 4 file(s) remaining \\rCompleted 1.6 MiB/2.3 MiB (7.0 MiB/s) with 4 file(s) remaining  \\rupload: train/train_part3.pdr to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/train/train_part3.pdr\\nCompleted 1.6 MiB/2.3 MiB (7.0 MiB/s) with 3 file(s) remaining\\rCompleted 1.8 MiB/2.3 MiB (7.9 MiB/s) with 3 file(s) remaining\\rCompleted 2.1 MiB/2.3 MiB (8.9 MiB/s) with 3 file(s) remaining\\rCompleted 2.1 MiB/2.3 MiB (9.3 MiB/s) with 3 file(s) remaining\\rupload: train/train_part1.pdr to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/train/train_part1.pdr\\nCompleted 2.1 MiB/2.3 MiB (9.3 MiB/s) with 2 file(s) remaining\\rCompleted 2.2 MiB/2.3 MiB (9.5 MiB/s) with 2 file(s) remaining\\rupload: train/train_part0.pdr to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/train/train_part0.pdr\\nCompleted 2.2 MiB/2.3 MiB (9.5 MiB/s) with 1 file(s) remaining\\rCompleted 2.3 MiB/2.3 MiB (6.7 MiB/s) with 1 file(s) remaining\\rupload: train/train_part2.pdr to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/train/train_part2.pdr\\n',\n",
              " None)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "cmd_train = \"aws s3 cp \" + train_data_dir + \" \" + s3_train_data + \" --recursive\"\n",
        "p = subprocess.Popen(cmd_train, shell=True, stdout=subprocess.PIPE)\n",
        "p.communicate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161ec67a-c4dd-4988-aff2-d13cddabb708",
      "metadata": {
        "tags": [],
        "id": "161ec67a-c4dd-4988-aff2-d13cddabb708",
        "outputId": "dd9f6447-2eee-4e44-ef4a-b475f2c4e987"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(b'Completed 238.8 KiB/238.8 KiB (2.0 MiB/s) with 1 file(s) remaining\\rupload: Validation/val_part0.pdr to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/val/val_part0.pdr\\n',\n",
              " None)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cmd_val = \"aws s3 cp \" + val_data_dir + \" \" + s3_val_data + \" --recursive\"\n",
        "p = subprocess.Popen(cmd_val, shell=True, stdout=subprocess.PIPE)\n",
        "p.communicate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db609cb-d4d3-4ca6-b800-86a0acdcc5e7",
      "metadata": {
        "tags": [],
        "id": "1db609cb-d4d3-4ca6-b800-86a0acdcc5e7",
        "outputId": "d7f255ad-762d-4aa0-ebc3-28056fb687c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(b'Completed 247.9 KiB/247.9 KiB (1.5 MiB/s) with 1 file(s) remaining\\rupload: Test/test_part0.pdr to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/test/test_part0.pdr\\n',\n",
              " None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cmd_test = \"aws s3 cp \" + test_data_dir + \" \" + s3_test_data + \" --recursive\"\n",
        "p = subprocess.Popen(cmd_test, shell=True, stdout=subprocess.PIPE)\n",
        "p.communicate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2e9b2f-da5a-4e51-ab31-0d9775aa992a",
      "metadata": {
        "tags": [],
        "id": "6c2e9b2f-da5a-4e51-ab31-0d9775aa992a",
        "outputId": "8971fc22-1325-4259-af71-05c7b6363758"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(b'Completed 164.1 KiB/164.1 KiB (1.2 MiB/s) with 1 file(s) remaining\\rupload: auxiliary/vocab.txt to s3://sagemaker-us-west-2-792421635322/ntm/wikitext-2/auxiliary/vocab.txt\\n',\n",
              " None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cmd_aux = \"aws s3 cp \" + aux_data_dir + \" \" + s3_aux_data + \" --recursive\"\n",
        "p = subprocess.Popen(cmd_aux, shell=True, stdout=subprocess.PIPE)\n",
        "p.communicate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6991ea63-37ba-4d73-b259-b83f1307ad2c",
      "metadata": {
        "tags": [],
        "id": "6991ea63-37ba-4d73-b259-b83f1307ad2c"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from sagemaker.image_uris import retrieve\n",
        "\n",
        "container = retrieve('ntm', boto3.Session().region_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf24a30-9cc8-43ee-9d26-92a29ed2e726",
      "metadata": {
        "tags": [],
        "id": "adf24a30-9cc8-43ee-9d26-92a29ed2e726"
      },
      "outputs": [],
      "source": [
        "sess = sagemaker.Session()\n",
        "# Create a SageMaker Estimator object for training a machine learning model\n",
        "# specifying the Docker container, IAM role, instance type, instance count,\n",
        "# output path for model artifacts, and the SageMaker session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ca466d-5c66-425f-8d47-a7f50e3a72ec",
      "metadata": {
        "tags": [],
        "id": "d3ca466d-5c66-425f-8d47-a7f50e3a72ec"
      },
      "outputs": [],
      "source": [
        "num_topics = 20\n",
        "# One line code to sets the hyperparameters for a Neural Topic Model (NTM),\n",
        "#   specifying the number of topics,\n",
        "#   input feature dimension (vocab_size)\n",
        "#   mini-batch size = 60\n",
        "#   number of training epochs = 50\n",
        "#   sub-sampling ratio = 0.7\n",
        "# for training.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d5052a-f9f6-42a2-b5a6-01bf4eb03c39",
      "metadata": {
        "tags": [],
        "id": "09d5052a-f9f6-42a2-b5a6-01bf4eb03c39"
      },
      "outputs": [],
      "source": [
        "from sagemaker.inputs import TrainingInput\n",
        "\n",
        "s3_train = TrainingInput(s3_train_data, distribution=\"ShardedByS3Key\", content_type=\"application/x-recordio-protobuf\")\n",
        "s3_val = TrainingInput(s3_val_data, distribution=\"FullyReplicated\",    content_type=\"application/x-recordio-protobuf\")\n",
        "s3_test = TrainingInput(s3_test_data, distribution=\"FullyReplicated\",  content_type=\"application/x-recordio-protobuf\")\n",
        "s3_aux = TrainingInput(s3_aux_data, distribution=\"FullyReplicated\",    content_type=\"application/x-recordio-protobuf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8f5c89-3085-4c84-9d76-5fbe2c01487b",
      "metadata": {
        "tags": [],
        "id": "ee8f5c89-3085-4c84-9d76-5fbe2c01487b",
        "outputId": "daad5529-f917-486a-9e8d-be4bf2ad54ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Creating training-job with name: ntm-2024-11-15-03-31-32-200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-11-15 03:31:33 Starting - Starting the training job...\n",
            "2024-11-15 03:31:47 Starting - Preparing the instances for training...\n",
            "2024-11-15 03:32:34 Downloading - Downloading the training image.....................\n",
            "2024-11-15 03:36:06 Training - Training image download completed. Training in progress...\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
            "\u001b[34mRunning default environment configuration script\u001b[0m\n",
            "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '50', 'feature_dim': '20439', 'mini_batch_size': '60', 'num_topics': '20', 'sub_sample': '0.7'}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '60', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '0.7', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '20439', 'num_topics': '20'}\u001b[0m\n",
            "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
            "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] nvidia-smi identified 0 GPUs.\u001b[0m\n",
            "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Using default worker.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Checkpoint loading and saving are disabled.\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:12.705] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:12.707] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Initializing\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] /opt/ml/input/data/auxiliary\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] vocab.txt\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:12 INFO 139684292245312] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.8/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 WARNING 139684292245312] 13 out of 20439 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Vocab embedding shape: (20439, 50)\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Number of GPUs being used: 0\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Create Store: local\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641783.3462634, \"EndTime\": 1731641783.346289, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:23.346] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 10644, \"num_examples\": 1, \"num_bytes\": 265704}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] # Starting training for epoch 1\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:23.892] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 544, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] # Finished training epoch 1 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) total: 9.782888067336309\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) kld: 0.009738687567767643\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) recons: 9.77314932686942\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) logppx: 9.782888067336309\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=9.782888067336309\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:23.899] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 11193, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:23.922] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) total: 9.166292317708333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) kld: 0.07198731104532878\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) recons: 9.094305419921875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Loss (name: value) logppx: 9.166292317708333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] #validation_score (1): 9.166292317708333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] Timing: train: 0.55s, val: 0.03s, epoch: 0.58s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641783.3466463, \"EndTime\": 1731641783.9251783, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1036.803527941692 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:23 INFO 139684292245312] # Starting training for epoch 2\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:24.329] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 404, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] # Finished training epoch 2 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) total: 9.170805528428819\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) kld: 0.05925665431552463\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) recons: 9.111549038357206\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) logppx: 9.170805528428819\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=9.170805528428819\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:24.352] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) total: 8.978177897135417\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) kld: 0.03797789812088013\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) recons: 8.940199788411459\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) logppx: 8.978177897135417\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #validation_score (2): 8.978177897135417\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Timing: train: 0.41s, val: 0.03s, epoch: 0.43s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641783.9254699, \"EndTime\": 1731641784.357385, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1200.0, \"count\": 1, \"min\": 1200, \"max\": 1200}, \"Total Batches Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1388.7243578636253 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] # Starting training for epoch 3\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:24.802] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 440, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] # Finished training epoch 3 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) total: 9.018357340494791\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) kld: 0.05404166636012849\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) recons: 8.964315359933035\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) logppx: 9.018357340494791\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=9.018357340494791\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:24.827] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) total: 8.892836507161459\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) kld: 0.06378486553827921\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) recons: 8.829050699869791\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Loss (name: value) logppx: 8.892836507161459\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #validation_score (3): 8.892836507161459\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] Timing: train: 0.45s, val: 0.03s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641784.357779, \"EndTime\": 1731641784.833175, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1800.0, \"count\": 1, \"min\": 1800, \"max\": 1800}, \"Total Batches Seen\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1261.5788745470697 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:24 INFO 139684292245312] # Starting training for epoch 4\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:25.384] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 548, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] # Finished training epoch 4 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) total: 8.908420435587566\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) kld: 0.05554130474726359\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) recons: 8.852879333496094\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) logppx: 8.908420435587566\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=8.908420435587566\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:25.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) total: 8.852327473958333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) kld: 0.042131157716115315\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) recons: 8.810195922851562\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Loss (name: value) logppx: 8.852327473958333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] #validation_score (4): 8.852327473958333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] patience losses:[9.166292317708333, 8.978177897135417, 8.892836507161459] min patience loss:8.892836507161459 current loss:8.852327473958333 absolute loss difference:0.04050903320312571\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] Timing: train: 0.55s, val: 0.03s, epoch: 0.58s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641784.8335133, \"EndTime\": 1731641785.414444, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2400.0, \"count\": 1, \"min\": 2400, \"max\": 2400}, \"Total Batches Seen\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1032.5542006531978 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:25 INFO 139684292245312] # Starting training for epoch 5\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:26.006] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 587, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] # Finished training epoch 5 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) total: 8.900806681315105\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) kld: 0.04987064252297083\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) recons: 8.850936126708984\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) logppx: 8.900806681315105\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=8.900806681315105\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:26.043] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 31, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) total: 8.849001057942708\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) kld: 0.03607083956400554\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) recons: 8.812930297851562\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) logppx: 8.849001057942708\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #validation_score (5): 8.849001057942708\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] patience losses:[8.978177897135417, 8.892836507161459, 8.852327473958333] min patience loss:8.852327473958333 current loss:8.849001057942708 absolute loss difference:0.003326416015625\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Timing: train: 0.59s, val: 0.04s, epoch: 0.63s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641785.414717, \"EndTime\": 1731641786.0491579, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3000.0, \"count\": 1, \"min\": 3000, \"max\": 3000}, \"Total Batches Seen\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=945.1462797421204 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] # Starting training for epoch 6\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:26.825] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 770, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] # Finished training epoch 6 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) total: 8.88254254659017\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) kld: 0.04806355138619741\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) recons: 8.834479141235352\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) logppx: 8.88254254659017\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=8.88254254659017\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:26.871] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 41, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) total: 8.844676717122395\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) kld: 0.0321750541528066\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) recons: 8.812501017252604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Loss (name: value) logppx: 8.844676717122395\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #validation_score (6): 8.844676717122395\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] patience losses:[8.892836507161459, 8.852327473958333, 8.849001057942708] min patience loss:8.849001057942708 current loss:8.844676717122395 absolute loss difference:0.004324340820312855\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] Timing: train: 0.78s, val: 0.05s, epoch: 0.82s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641786.0498748, \"EndTime\": 1731641786.8745906, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3600.0, \"count\": 1, \"min\": 3600, \"max\": 3600}, \"Total Batches Seen\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=727.4120587182821 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:26 INFO 139684292245312] # Starting training for epoch 7\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:27.541] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 658, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] # Finished training epoch 7 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) total: 8.896898651123047\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) kld: 0.042333170771598816\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) recons: 8.854565556844076\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) logppx: 8.896898651123047\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=8.896898651123047\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:27.565] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) total: 8.827077229817709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) kld: 0.03532736698786418\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) recons: 8.791750081380208\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Loss (name: value) logppx: 8.827077229817709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] #validation_score (7): 8.827077229817709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] patience losses:[8.852327473958333, 8.849001057942708, 8.844676717122395] min patience loss:8.844676717122395 current loss:8.827077229817709 absolute loss difference:0.01759948730468608\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] Timing: train: 0.67s, val: 0.03s, epoch: 0.70s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641786.8747935, \"EndTime\": 1731641787.5708175, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4200.0, \"count\": 1, \"min\": 4200, \"max\": 4200}, \"Total Batches Seen\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=861.8058050214939 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:27 INFO 139684292245312] # Starting training for epoch 8\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:28.064] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 489, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] # Finished training epoch 8 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) total: 8.90326160249256\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) kld: 0.04142920005889166\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) recons: 8.861832391648065\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) logppx: 8.90326160249256\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=8.90326160249256\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:28.087] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) total: 8.86426493326823\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) kld: 0.02597480614980062\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) recons: 8.838290405273437\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) logppx: 8.86426493326823\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #validation_score (8): 8.86426493326823\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] patience losses:[8.849001057942708, 8.844676717122395, 8.827077229817709] min patience loss:8.827077229817709 current loss:8.86426493326823 absolute loss difference:0.03718770345052036\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Timing: train: 0.49s, val: 0.02s, epoch: 0.52s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641787.5711846, \"EndTime\": 1731641788.089205, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4800.0, \"count\": 1, \"min\": 4800, \"max\": 4800}, \"Total Batches Seen\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1157.9938763846926 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] # Starting training for epoch 9\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:28.689] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 599, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] # Finished training epoch 9 on 540 examples from 9 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Subsampled 9 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) total: 8.869819584599247\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) kld: 0.0399008278493528\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) recons: 8.829918755425346\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) logppx: 8.869819584599247\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=8.869819584599247\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:28.713] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) total: 8.817458089192709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) kld: 0.040932750701904295\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) recons: 8.776524861653646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Loss (name: value) logppx: 8.817458089192709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #validation_score (9): 8.817458089192709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] patience losses:[8.844676717122395, 8.827077229817709, 8.86426493326823] min patience loss:8.827077229817709 current loss:8.817458089192709 absolute loss difference:0.009619140625000355\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] Timing: train: 0.60s, val: 0.03s, epoch: 0.63s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641788.089439, \"EndTime\": 1731641788.71921, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5400.0, \"count\": 1, \"min\": 5400, \"max\": 5400}, \"Total Batches Seen\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=952.3757335115534 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:28 INFO 139684292245312] # Starting training for epoch 10\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:29.312] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 590, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] # Finished training epoch 10 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) total: 8.87848269144694\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) kld: 0.03842014422019323\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) recons: 8.840062459309896\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) logppx: 8.87848269144694\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=8.87848269144694\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:29.336] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) total: 8.822480265299479\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) kld: 0.04112371603647868\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) recons: 8.781355794270834\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) logppx: 8.822480265299479\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #validation_score (10): 8.822480265299479\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] patience losses:[8.827077229817709, 8.86426493326823, 8.817458089192709] min patience loss:8.817458089192709 current loss:8.822480265299479 absolute loss difference:0.00502217610677036\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Timing: train: 0.59s, val: 0.02s, epoch: 0.62s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641788.7196927, \"EndTime\": 1731641789.3373826, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6000.0, \"count\": 1, \"min\": 6000, \"max\": 6000}, \"Total Batches Seen\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=971.0756650193842 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] # Starting training for epoch 11\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:29.784] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 446, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] # Finished training epoch 11 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) total: 8.86479986281622\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) kld: 0.039256384259178524\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) recons: 8.825543503534226\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) logppx: 8.86479986281622\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=8.86479986281622\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:29.805] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) total: 8.825701904296874\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) kld: 0.041639486948649086\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) recons: 8.784062703450521\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Loss (name: value) logppx: 8.825701904296874\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #validation_score (11): 8.825701904296874\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] patience losses:[8.86426493326823, 8.817458089192709, 8.822480265299479] min patience loss:8.817458089192709 current loss:8.825701904296874 absolute loss difference:0.00824381510416572\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] Timing: train: 0.45s, val: 0.02s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641789.3376753, \"EndTime\": 1731641789.8067877, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6600.0, \"count\": 1, \"min\": 6600, \"max\": 6600}, \"Total Batches Seen\": {\"sum\": 110.0, \"count\": 1, \"min\": 110, \"max\": 110}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1278.6940421363272 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:29 INFO 139684292245312] # Starting training for epoch 12\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:30.140] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 333, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] # Finished training epoch 12 on 300 examples from 5 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Subsampled 5 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) total: 8.83978983561198\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) kld: 0.03948993404706319\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) recons: 8.800299886067709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) logppx: 8.83978983561198\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=8.83978983561198\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:30.163] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) total: 8.812120564778645\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) kld: 0.0294644037882487\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) recons: 8.782655843098958\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) logppx: 8.812120564778645\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #validation_score (12): 8.812120564778645\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] patience losses:[8.817458089192709, 8.822480265299479, 8.825701904296874] min patience loss:8.817458089192709 current loss:8.812120564778645 absolute loss difference:0.0053375244140632105\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Timing: train: 0.34s, val: 0.03s, epoch: 0.36s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641789.8071084, \"EndTime\": 1731641790.1692169, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7200.0, \"count\": 1, \"min\": 7200, \"max\": 7200}, \"Total Batches Seen\": {\"sum\": 120.0, \"count\": 1, \"min\": 120, \"max\": 120}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1655.719298153274 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] # Starting training for epoch 13\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:30.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 652, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] # Finished training epoch 13 on 600 examples from 10 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Subsampled 10 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) total: 8.868212687174479\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) kld: 0.03641766270001729\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) recons: 8.831795145670572\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) logppx: 8.868212687174479\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=8.868212687174479\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:30.846] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) total: 8.830959065755208\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) kld: 0.023962515592575073\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) recons: 8.806996663411459\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Loss (name: value) logppx: 8.830959065755208\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #validation_score (13): 8.830959065755208\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] patience losses:[8.822480265299479, 8.825701904296874, 8.812120564778645] min patience loss:8.812120564778645 current loss:8.830959065755208 absolute loss difference:0.018838500976562145\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] Timing: train: 0.66s, val: 0.02s, epoch: 0.68s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641790.1697574, \"EndTime\": 1731641790.8479917, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7800.0, \"count\": 1, \"min\": 7800, \"max\": 7800}, \"Total Batches Seen\": {\"sum\": 130.0, \"count\": 1, \"min\": 130, \"max\": 130}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=884.4796045646299 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:30 INFO 139684292245312] # Starting training for epoch 14\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:31.228] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 380, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] # Finished training epoch 14 on 300 examples from 5 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Subsampled 5 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) total: 8.877847086588542\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) kld: 0.032361824512481686\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) recons: 8.845485229492187\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) logppx: 8.877847086588542\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=8.877847086588542\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:31.251] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) total: 8.803126017252604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) kld: 0.03714884519577026\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) recons: 8.765976969401041\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) logppx: 8.803126017252604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #validation_score (14): 8.803126017252604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] patience losses:[8.825701904296874, 8.812120564778645, 8.830959065755208] min patience loss:8.812120564778645 current loss:8.803126017252604 absolute loss difference:0.008994547526041785\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Timing: train: 0.38s, val: 0.02s, epoch: 0.41s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641790.8482313, \"EndTime\": 1731641791.2549067, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 8400.0, \"count\": 1, \"min\": 8400, \"max\": 8400}, \"Total Batches Seen\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1474.7550017844155 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] # Starting training for epoch 15\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:31.699] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 440, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] # Finished training epoch 15 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) total: 8.834221976143974\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) kld: 0.038091626053764704\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) recons: 8.79613022577195\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) logppx: 8.834221976143974\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=8.834221976143974\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:31.722] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) total: 8.799349975585937\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) kld: 0.02729015549023946\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) recons: 8.772060139973958\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Loss (name: value) logppx: 8.799349975585937\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #validation_score (15): 8.799349975585937\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] patience losses:[8.812120564778645, 8.830959065755208, 8.803126017252604] min patience loss:8.803126017252604 current loss:8.799349975585937 absolute loss difference:0.003776041666666785\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] Timing: train: 0.45s, val: 0.03s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641791.2552452, \"EndTime\": 1731641791.7260456, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9000.0, \"count\": 1, \"min\": 9000, \"max\": 9000}, \"Total Batches Seen\": {\"sum\": 150.0, \"count\": 1, \"min\": 150, \"max\": 150}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1273.9518905910356 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:31 INFO 139684292245312] # Starting training for epoch 16\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:32.264] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 534, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] # Finished training epoch 16 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) total: 8.82864875793457\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) kld: 0.034633556753396987\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) recons: 8.794015248616537\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) logppx: 8.82864875793457\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=8.82864875793457\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:32.296] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 30, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) total: 8.782164510091146\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) kld: 0.027569574117660523\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) recons: 8.75459493001302\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) logppx: 8.782164510091146\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #validation_score (16): 8.782164510091146\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] patience losses:[8.830959065755208, 8.803126017252604, 8.799349975585937] min patience loss:8.799349975585937 current loss:8.782164510091146 absolute loss difference:0.01718546549479072\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Timing: train: 0.54s, val: 0.04s, epoch: 0.58s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641791.7263782, \"EndTime\": 1731641792.3049731, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9600.0, \"count\": 1, \"min\": 9600, \"max\": 9600}, \"Total Batches Seen\": {\"sum\": 160.0, \"count\": 1, \"min\": 160, \"max\": 160}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1035.948649256109 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] # Starting training for epoch 17\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:32.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 451, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] # Finished training epoch 17 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) total: 8.824395461309523\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) kld: 0.034900020417712986\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) recons: 8.789495413643973\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) logppx: 8.824395461309523\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=8.824395461309523\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:32.779] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) total: 8.793831380208333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) kld: 0.026398317019144694\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) recons: 8.767433675130208\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Loss (name: value) logppx: 8.793831380208333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #validation_score (17): 8.793831380208333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] patience losses:[8.803126017252604, 8.799349975585937, 8.782164510091146] min patience loss:8.782164510091146 current loss:8.793831380208333 absolute loss difference:0.011666870117187145\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] Timing: train: 0.45s, val: 0.02s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641792.3059716, \"EndTime\": 1731641792.779872, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 10200.0, \"count\": 1, \"min\": 10200, \"max\": 10200}, \"Total Batches Seen\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1265.7871917495409 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:32 INFO 139684292245312] # Starting training for epoch 18\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:33.309] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 529, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] # Finished training epoch 18 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) total: 8.802927017211914\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) kld: 0.03757408062616984\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) recons: 8.76535250345866\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) logppx: 8.802927017211914\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=8.802927017211914\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:33.334] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) total: 8.76033935546875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) kld: 0.0292350172996521\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) recons: 8.731104532877604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) logppx: 8.76033935546875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #validation_score (18): 8.76033935546875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] patience losses:[8.799349975585937, 8.782164510091146, 8.793831380208333] min patience loss:8.782164510091146 current loss:8.76033935546875 absolute loss difference:0.021825154622396425\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Timing: train: 0.53s, val: 0.03s, epoch: 0.56s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641792.7800562, \"EndTime\": 1731641793.3379238, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 10800.0, \"count\": 1, \"min\": 10800, \"max\": 10800}, \"Total Batches Seen\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1075.1730944232963 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] # Starting training for epoch 19\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:33.866] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 524, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] # Finished training epoch 19 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) total: 8.753404744466145\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) kld: 0.03897902667522431\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) recons: 8.714425786336262\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) logppx: 8.753404744466145\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=8.753404744466145\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:33.890] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) total: 8.724950154622396\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) kld: 0.035096510251363115\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) recons: 8.689853922526042\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Loss (name: value) logppx: 8.724950154622396\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #validation_score (19): 8.724950154622396\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] patience losses:[8.782164510091146, 8.793831380208333, 8.76033935546875] min patience loss:8.76033935546875 current loss:8.724950154622396 absolute loss difference:0.03538920084635322\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] Timing: train: 0.53s, val: 0.03s, epoch: 0.56s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641793.338272, \"EndTime\": 1731641793.8938816, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 11400.0, \"count\": 1, \"min\": 11400, \"max\": 11400}, \"Total Batches Seen\": {\"sum\": 190.0, \"count\": 1, \"min\": 190, \"max\": 190}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1079.5463865872093 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:33 INFO 139684292245312] # Starting training for epoch 20\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:34.344] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 445, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] # Finished training epoch 20 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) total: 8.737772623697916\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) kld: 0.03936997453371684\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) recons: 8.698402913411458\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) logppx: 8.737772623697916\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=8.737772623697916\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:34.370] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) total: 8.720436604817708\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) kld: 0.035276679197947185\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) recons: 8.685160319010416\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) logppx: 8.720436604817708\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #validation_score (20): 8.720436604817708\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] patience losses:[8.793831380208333, 8.76033935546875, 8.724950154622396] min patience loss:8.724950154622396 current loss:8.720436604817708 absolute loss difference:0.0045135498046882105\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Timing: train: 0.45s, val: 0.03s, epoch: 0.48s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641793.8942409, \"EndTime\": 1731641794.3757443, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 12000.0, \"count\": 1, \"min\": 12000, \"max\": 12000}, \"Total Batches Seen\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1245.5572455006243 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] # Starting training for epoch 21\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:34.909] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 531, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] # Finished training epoch 21 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) total: 8.74607187906901\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) kld: 0.04095688213904699\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) recons: 8.705115127563477\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) logppx: 8.74607187906901\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=8.74607187906901\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:34.932] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) total: 8.729842122395834\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) kld: 0.04149632851282756\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) recons: 8.688345336914063\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Loss (name: value) logppx: 8.729842122395834\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #validation_score (21): 8.729842122395834\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] patience losses:[8.76033935546875, 8.724950154622396, 8.720436604817708] min patience loss:8.720436604817708 current loss:8.729842122395834 absolute loss difference:0.009405517578125355\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] Timing: train: 0.53s, val: 0.02s, epoch: 0.56s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641794.3761792, \"EndTime\": 1731641794.9337656, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 12600.0, \"count\": 1, \"min\": 12600, \"max\": 12600}, \"Total Batches Seen\": {\"sum\": 210.0, \"count\": 1, \"min\": 210, \"max\": 210}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1075.7889787072145 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:34 INFO 139684292245312] # Starting training for epoch 22\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:35.447] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 513, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] # Finished training epoch 22 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) total: 8.72886454264323\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) kld: 0.042236779381831485\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) recons: 8.686627705891928\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) logppx: 8.72886454264323\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=8.72886454264323\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:35.472] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) total: 8.691645304361979\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) kld: 0.03886873324712117\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) recons: 8.652777099609375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) logppx: 8.691645304361979\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #validation_score (22): 8.691645304361979\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] patience losses:[8.724950154622396, 8.720436604817708, 8.729842122395834] min patience loss:8.720436604817708 current loss:8.691645304361979 absolute loss difference:0.02879130045572964\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Timing: train: 0.52s, val: 0.03s, epoch: 0.54s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641794.934059, \"EndTime\": 1731641795.4770792, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13200.0, \"count\": 1, \"min\": 13200, \"max\": 13200}, \"Total Batches Seen\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1104.5029679939432 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] # Starting training for epoch 23\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:35.893] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 413, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] # Finished training epoch 23 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) total: 8.712469482421875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) kld: 0.04001598589950138\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) recons: 8.672453477647569\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) logppx: 8.712469482421875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=8.712469482421875\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:35.915] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) total: 8.686452229817709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) kld: 0.03887984752655029\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) recons: 8.647572835286459\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Loss (name: value) logppx: 8.686452229817709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #validation_score (23): 8.686452229817709\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] patience losses:[8.720436604817708, 8.729842122395834, 8.691645304361979] min patience loss:8.691645304361979 current loss:8.686452229817709 absolute loss difference:0.005193074544269649\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] Timing: train: 0.42s, val: 0.02s, epoch: 0.44s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641795.4775057, \"EndTime\": 1731641795.91885, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13800.0, \"count\": 1, \"min\": 13800, \"max\": 13800}, \"Total Batches Seen\": {\"sum\": 230.0, \"count\": 1, \"min\": 230, \"max\": 230}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1359.0219034864128 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:35 INFO 139684292245312] # Starting training for epoch 24\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:36.450] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 527, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] # Finished training epoch 24 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) total: 8.671411895751953\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) kld: 0.044132167597611745\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) recons: 8.627279663085938\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) logppx: 8.671411895751953\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=8.671411895751953\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:36.475] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) total: 8.678955078125\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) kld: 0.03811079263687134\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) recons: 8.6408447265625\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) logppx: 8.678955078125\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #validation_score (24): 8.678955078125\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] patience losses:[8.729842122395834, 8.691645304361979, 8.686452229817709] min patience loss:8.686452229817709 current loss:8.678955078125 absolute loss difference:0.0074971516927089255\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Timing: train: 0.53s, val: 0.03s, epoch: 0.56s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641795.919129, \"EndTime\": 1731641796.478812, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 14400.0, \"count\": 1, \"min\": 14400, \"max\": 14400}, \"Total Batches Seen\": {\"sum\": 240.0, \"count\": 1, \"min\": 240, \"max\": 240}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1071.7343940310202 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] # Starting training for epoch 25\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:36.928] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 446, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] # Finished training epoch 25 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) total: 8.678157842726934\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) kld: 0.04354965119134812\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) recons: 8.634608314150857\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) logppx: 8.678157842726934\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=8.678157842726934\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:36.952] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 74, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) total: 8.679108683268229\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) kld: 0.03423701127370198\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) recons: 8.644871012369792\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Loss (name: value) logppx: 8.679108683268229\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #validation_score (25): 8.679108683268229\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] patience losses:[8.691645304361979, 8.686452229817709, 8.678955078125] min patience loss:8.678955078125 current loss:8.679108683268229 absolute loss difference:0.00015360514322892982\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] Timing: train: 0.45s, val: 0.02s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641796.479123, \"EndTime\": 1731641796.9532542, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 15000.0, \"count\": 1, \"min\": 15000, \"max\": 15000}, \"Total Batches Seen\": {\"sum\": 250.0, \"count\": 1, \"min\": 250, \"max\": 250}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1265.1635685041144 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:36 INFO 139684292245312] # Starting training for epoch 26\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:37.340] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 386, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] # Finished training epoch 26 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) total: 8.654852718777127\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) kld: 0.04388834171824985\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) recons: 8.61096445719401\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) logppx: 8.654852718777127\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=8.654852718777127\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:37.362] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 77, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) total: 8.662825520833334\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) kld: 0.03999242782592773\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) recons: 8.622833251953125\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) logppx: 8.662825520833334\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #validation_score (26): 8.662825520833334\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] patience losses:[8.686452229817709, 8.678955078125, 8.679108683268229] min patience loss:8.678955078125 current loss:8.662825520833334 absolute loss difference:0.01612955729166643\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Timing: train: 0.39s, val: 0.03s, epoch: 0.41s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641796.9534633, \"EndTime\": 1731641797.367345, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 15600.0, \"count\": 1, \"min\": 15600, \"max\": 15600}, \"Total Batches Seen\": {\"sum\": 260.0, \"count\": 1, \"min\": 260, \"max\": 260}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1448.9289561547093 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] # Starting training for epoch 27\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:37.760] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 390, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] # Finished training epoch 27 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) total: 8.648917219373915\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) kld: 0.0449200967947642\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) recons: 8.603997039794923\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) logppx: 8.648917219373915\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=8.648917219373915\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:37.781] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 80, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) total: 8.66252950032552\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) kld: 0.04562150239944458\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) recons: 8.616907755533854\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Loss (name: value) logppx: 8.66252950032552\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #validation_score (27): 8.66252950032552\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] patience losses:[8.678955078125, 8.679108683268229, 8.662825520833334] min patience loss:8.662825520833334 current loss:8.66252950032552 absolute loss difference:0.00029602050781285527\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] Timing: train: 0.39s, val: 0.02s, epoch: 0.42s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641797.36776, \"EndTime\": 1731641797.7857091, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 16200.0, \"count\": 1, \"min\": 16200, \"max\": 16200}, \"Total Batches Seen\": {\"sum\": 270.0, \"count\": 1, \"min\": 270, \"max\": 270}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1435.1276260863615 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:37 INFO 139684292245312] # Starting training for epoch 28\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:38.402] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 613, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] # Finished training epoch 28 on 540 examples from 9 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Subsampled 9 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) total: 8.621760728624132\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) kld: 0.046735098406120584\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) recons: 8.575025431315105\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) logppx: 8.621760728624132\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=8.621760728624132\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:38.426] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 83, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) total: 8.624425252278646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) kld: 0.035542384783426924\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) recons: 8.588882446289062\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) logppx: 8.624425252278646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #validation_score (28): 8.624425252278646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] patience losses:[8.679108683268229, 8.662825520833334, 8.66252950032552] min patience loss:8.66252950032552 current loss:8.624425252278646 absolute loss difference:0.03810424804687429\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Timing: train: 0.62s, val: 0.03s, epoch: 0.64s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641797.7859666, \"EndTime\": 1731641798.4314857, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 16800.0, \"count\": 1, \"min\": 16800, \"max\": 16800}, \"Total Batches Seen\": {\"sum\": 280.0, \"count\": 1, \"min\": 280, \"max\": 280}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=929.1935941833241 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] # Starting training for epoch 29\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:38.854] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 420, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] # Finished training epoch 29 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) total: 8.602955457899306\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) kld: 0.04605872299936083\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) recons: 8.556896718343099\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) logppx: 8.602955457899306\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=8.602955457899306\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:38.878] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 86, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) total: 8.606387329101562\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) kld: 0.03831616640090942\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) recons: 8.56807149251302\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Loss (name: value) logppx: 8.606387329101562\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #validation_score (29): 8.606387329101562\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] patience losses:[8.662825520833334, 8.66252950032552, 8.624425252278646] min patience loss:8.624425252278646 current loss:8.606387329101562 absolute loss difference:0.018037923177084636\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] Timing: train: 0.42s, val: 0.03s, epoch: 0.45s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641798.4318821, \"EndTime\": 1731641798.8823326, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 17400.0, \"count\": 1, \"min\": 17400, \"max\": 17400}, \"Total Batches Seen\": {\"sum\": 290.0, \"count\": 1, \"min\": 290, \"max\": 290}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1331.4116628160248 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:38 INFO 139684292245312] # Starting training for epoch 30\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:39.392] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 505, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] # Finished training epoch 30 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) total: 8.568745104471843\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) kld: 0.048694901665051776\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) recons: 8.520049985249837\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) logppx: 8.568745104471843\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=8.568745104471843\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:39.415] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 89, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) total: 8.585054524739583\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) kld: 0.04591128826141357\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) recons: 8.539143880208334\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) logppx: 8.585054524739583\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #validation_score (30): 8.585054524739583\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] patience losses:[8.66252950032552, 8.624425252278646, 8.606387329101562] min patience loss:8.606387329101562 current loss:8.585054524739583 absolute loss difference:0.02133280436197893\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Timing: train: 0.51s, val: 0.03s, epoch: 0.54s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641798.882766, \"EndTime\": 1731641799.4206684, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18000.0, \"count\": 1, \"min\": 18000, \"max\": 18000}, \"Total Batches Seen\": {\"sum\": 300.0, \"count\": 1, \"min\": 300, \"max\": 300}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1114.973113479856 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] # Starting training for epoch 31\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:39.887] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 463, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] # Finished training epoch 31 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) total: 8.519532703218006\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) kld: 0.05102162247612363\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) recons: 8.468511308942523\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) logppx: 8.519532703218006\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=8.519532703218006\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:39.910] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 92, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) total: 8.551258341471355\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) kld: 0.04156347910563151\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) recons: 8.509694925944011\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Loss (name: value) logppx: 8.551258341471355\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #validation_score (31): 8.551258341471355\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] patience losses:[8.624425252278646, 8.606387329101562, 8.585054524739583] min patience loss:8.585054524739583 current loss:8.551258341471355 absolute loss difference:0.033796183268227864\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] Timing: train: 0.47s, val: 0.03s, epoch: 0.49s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641799.4210954, \"EndTime\": 1731641799.91649, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18600.0, \"count\": 1, \"min\": 18600, \"max\": 18600}, \"Total Batches Seen\": {\"sum\": 310.0, \"count\": 1, \"min\": 310, \"max\": 310}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1210.620010535152 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:39 INFO 139684292245312] # Starting training for epoch 32\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:40.208] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 289, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] # Finished training epoch 32 on 240 examples from 4 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Subsampled 4 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) total: 8.550570551554362\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) kld: 0.04732081989447276\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) recons: 8.503249740600586\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) logppx: 8.550570551554362\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=8.550570551554362\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:40.230] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 95, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) total: 8.548040771484375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) kld: 0.046628570556640624\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) recons: 8.501412455240885\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) logppx: 8.548040771484375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #validation_score (32): 8.548040771484375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] patience losses:[8.606387329101562, 8.585054524739583, 8.551258341471355] min patience loss:8.551258341471355 current loss:8.548040771484375 absolute loss difference:0.0032175699869796404\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Timing: train: 0.29s, val: 0.02s, epoch: 0.32s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641799.916895, \"EndTime\": 1731641800.2338703, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 19200.0, \"count\": 1, \"min\": 19200, \"max\": 19200}, \"Total Batches Seen\": {\"sum\": 320.0, \"count\": 1, \"min\": 320, \"max\": 320}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1891.521540767287 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] # Starting training for epoch 33\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:40.679] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 441, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] # Finished training epoch 33 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) total: 8.560182480585008\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) kld: 0.048881055059887116\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) recons: 8.51130145844959\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) logppx: 8.560182480585008\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=8.560182480585008\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:40.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 98, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) total: 8.533354695638021\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) kld: 0.04280853271484375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) recons: 8.490546162923177\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Loss (name: value) logppx: 8.533354695638021\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #validation_score (33): 8.533354695638021\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] patience losses:[8.585054524739583, 8.551258341471355, 8.548040771484375] min patience loss:8.548040771484375 current loss:8.533354695638021 absolute loss difference:0.014686075846354285\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] Timing: train: 0.45s, val: 0.03s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641800.2342577, \"EndTime\": 1731641800.7082748, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 19800.0, \"count\": 1, \"min\": 19800, \"max\": 19800}, \"Total Batches Seen\": {\"sum\": 330.0, \"count\": 1, \"min\": 330, \"max\": 330}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1265.3715867134551 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:40 INFO 139684292245312] # Starting training for epoch 34\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:41.376] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 664, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] # Finished training epoch 34 on 600 examples from 10 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Subsampled 10 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) total: 8.502154744466146\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) kld: 0.05250558614730835\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) recons: 8.449649098714193\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) logppx: 8.502154744466146\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=8.502154744466146\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:41.401] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 101, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) total: 8.510707092285156\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) kld: 0.04443185329437256\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) recons: 8.466275533040365\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) logppx: 8.510707092285156\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #validation_score (34): 8.510707092285156\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] patience losses:[8.551258341471355, 8.548040771484375, 8.533354695638021] min patience loss:8.533354695638021 current loss:8.510707092285156 absolute loss difference:0.02264760335286553\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Timing: train: 0.67s, val: 0.03s, epoch: 0.70s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641800.7086067, \"EndTime\": 1731641801.4070835, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 20400.0, \"count\": 1, \"min\": 20400, \"max\": 20400}, \"Total Batches Seen\": {\"sum\": 340.0, \"count\": 1, \"min\": 340, \"max\": 340}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=858.7491439235549 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] # Starting training for epoch 35\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:41.868] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 458, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] # Finished training epoch 35 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) total: 8.477244131905692\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) kld: 0.051554885932377406\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) recons: 8.425689333961124\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) logppx: 8.477244131905692\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=8.477244131905692\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:41.891] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 104, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) total: 8.527519226074219\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) kld: 0.05438962777455648\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) recons: 8.47312978108724\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Loss (name: value) logppx: 8.527519226074219\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #validation_score (35): 8.527519226074219\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] patience losses:[8.548040771484375, 8.533354695638021, 8.510707092285156] min patience loss:8.510707092285156 current loss:8.527519226074219 absolute loss difference:0.01681213378906321\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] Timing: train: 0.46s, val: 0.02s, epoch: 0.48s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641801.4075198, \"EndTime\": 1731641801.8924277, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 21000.0, \"count\": 1, \"min\": 21000, \"max\": 21000}, \"Total Batches Seen\": {\"sum\": 350.0, \"count\": 1, \"min\": 350, \"max\": 350}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1236.9537478495945 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:41 INFO 139684292245312] # Starting training for epoch 36\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:42.227] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 334, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] # Finished training epoch 36 on 300 examples from 5 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Subsampled 5 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) total: 8.47889394124349\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) kld: 0.055209630330403645\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) recons: 8.42368408203125\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) logppx: 8.47889394124349\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=8.47889394124349\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:42.250] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 107, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) total: 8.502519226074218\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) kld: 0.04527389605840047\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) recons: 8.457245381673177\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) logppx: 8.502519226074218\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #validation_score (36): 8.502519226074218\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] patience losses:[8.533354695638021, 8.510707092285156, 8.527519226074219] min patience loss:8.510707092285156 current loss:8.502519226074218 absolute loss difference:0.008187866210937145\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Timing: train: 0.34s, val: 0.03s, epoch: 0.36s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641801.8926995, \"EndTime\": 1731641802.2542722, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 21600.0, \"count\": 1, \"min\": 21600, \"max\": 21600}, \"Total Batches Seen\": {\"sum\": 360.0, \"count\": 1, \"min\": 360, \"max\": 360}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1658.4722652414312 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] # Starting training for epoch 37\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:42.695] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 437, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] # Finished training epoch 37 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) total: 8.477920968191965\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) kld: 0.05400432177952358\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) recons: 8.423916698637463\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) logppx: 8.477920968191965\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=8.477920968191965\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:42.717] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 110, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) total: 8.491981506347656\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) kld: 0.04981398582458496\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) recons: 8.442167663574219\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Loss (name: value) logppx: 8.491981506347656\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #validation_score (37): 8.491981506347656\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] patience losses:[8.510707092285156, 8.527519226074219, 8.502519226074218] min patience loss:8.502519226074218 current loss:8.491981506347656 absolute loss difference:0.010537719726562145\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] Timing: train: 0.44s, val: 0.03s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641802.254642, \"EndTime\": 1731641802.7211604, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22200.0, \"count\": 1, \"min\": 22200, \"max\": 22200}, \"Total Batches Seen\": {\"sum\": 370.0, \"count\": 1, \"min\": 370, \"max\": 370}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1285.6701895209433 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:42 INFO 139684292245312] # Starting training for epoch 38\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:43.174] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 450, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] # Finished training epoch 38 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) total: 8.416911897205171\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) kld: 0.05800417888732184\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) recons: 8.358907717750187\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) logppx: 8.416911897205171\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=8.416911897205171\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:43.198] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 113, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) total: 8.468518575032553\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) kld: 0.04777568578720093\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) recons: 8.420742797851563\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) logppx: 8.468518575032553\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #validation_score (38): 8.468518575032553\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] patience losses:[8.527519226074219, 8.502519226074218, 8.491981506347656] min patience loss:8.491981506347656 current loss:8.468518575032553 absolute loss difference:0.023462931315103575\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Timing: train: 0.45s, val: 0.03s, epoch: 0.48s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641802.721438, \"EndTime\": 1731641803.203682, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22800.0, \"count\": 1, \"min\": 22800, \"max\": 22800}, \"Total Batches Seen\": {\"sum\": 380.0, \"count\": 1, \"min\": 380, \"max\": 380}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1243.6669206797271 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] # Starting training for epoch 39\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:43.619] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 412, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] # Finished training epoch 39 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) total: 8.459183078342013\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) kld: 0.05432053804397583\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) recons: 8.404862636990018\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) logppx: 8.459183078342013\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=8.459183078342013\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:43.642] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 116, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) total: 8.471062723795573\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) kld: 0.05295713742574056\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) recons: 8.41810557047526\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Loss (name: value) logppx: 8.471062723795573\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #validation_score (39): 8.471062723795573\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] patience losses:[8.502519226074218, 8.491981506347656, 8.468518575032553] min patience loss:8.468518575032553 current loss:8.471062723795573 absolute loss difference:0.0025441487630200044\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] Timing: train: 0.42s, val: 0.02s, epoch: 0.44s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641803.2040982, \"EndTime\": 1731641803.643612, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 23400.0, \"count\": 1, \"min\": 23400, \"max\": 23400}, \"Total Batches Seen\": {\"sum\": 390.0, \"count\": 1, \"min\": 390, \"max\": 390}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1364.763201136678 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:43 INFO 139684292245312] # Starting training for epoch 40\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:44.084] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 440, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] # Finished training epoch 40 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) total: 8.42181171235584\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) kld: 0.05606225104559036\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) recons: 8.36574946812221\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) logppx: 8.42181171235584\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=8.42181171235584\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:44.108] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 119, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) total: 8.460676574707032\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) kld: 0.051549057165781655\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) recons: 8.409127298990885\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) logppx: 8.460676574707032\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #validation_score (40): 8.460676574707032\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] patience losses:[8.491981506347656, 8.468518575032553, 8.471062723795573] min patience loss:8.468518575032553 current loss:8.460676574707032 absolute loss difference:0.00784200032552107\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Timing: train: 0.44s, val: 0.03s, epoch: 0.47s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641803.6437953, \"EndTime\": 1731641804.111501, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 24000.0, \"count\": 1, \"min\": 24000, \"max\": 24000}, \"Total Batches Seen\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1282.466331583515 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] # Starting training for epoch 41\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:44.674] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 559, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] # Finished training epoch 41 on 540 examples from 9 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Subsampled 9 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) total: 8.404507502803096\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) kld: 0.060449643929799395\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) recons: 8.344057888454861\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) logppx: 8.404507502803096\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=8.404507502803096\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:44.696] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 122, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) total: 8.44919942220052\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) kld: 0.055058062076568604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) recons: 8.394141642252604\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Loss (name: value) logppx: 8.44919942220052\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #validation_score (41): 8.44919942220052\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] patience losses:[8.468518575032553, 8.471062723795573, 8.460676574707032] min patience loss:8.460676574707032 current loss:8.44919942220052 absolute loss difference:0.011477152506511601\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] Timing: train: 0.56s, val: 0.02s, epoch: 0.59s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641804.1117766, \"EndTime\": 1731641804.7001512, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 24600.0, \"count\": 1, \"min\": 24600, \"max\": 24600}, \"Total Batches Seen\": {\"sum\": 410.0, \"count\": 1, \"min\": 410, \"max\": 410}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1019.4135515788544 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:44 INFO 139684292245312] # Starting training for epoch 42\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:45.224] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 520, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] # Finished training epoch 42 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) total: 8.389130592346191\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) kld: 0.06035432269175847\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) recons: 8.328776295979818\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) logppx: 8.389130592346191\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=8.389130592346191\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:45.247] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 125, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) total: 8.435460408528646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) kld: 0.0543678879737854\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) recons: 8.381092834472657\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) logppx: 8.435460408528646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #validation_score (42): 8.435460408528646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] patience losses:[8.471062723795573, 8.460676574707032, 8.44919942220052] min patience loss:8.44919942220052 current loss:8.435460408528646 absolute loss difference:0.013739013671873579\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Timing: train: 0.53s, val: 0.03s, epoch: 0.55s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641804.7005243, \"EndTime\": 1731641805.2511358, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 25200.0, \"count\": 1, \"min\": 25200, \"max\": 25200}, \"Total Batches Seen\": {\"sum\": 420.0, \"count\": 1, \"min\": 420, \"max\": 420}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1089.412632530402 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] # Starting training for epoch 43\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:45.746] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 492, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] # Finished training epoch 43 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) total: 8.367995834350586\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) kld: 0.06241496056318283\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) recons: 8.305580774943033\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) logppx: 8.367995834350586\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=8.367995834350586\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:45.768] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 128, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) total: 8.422337849934896\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) kld: 0.05532842477162679\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) recons: 8.367009480794271\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Loss (name: value) logppx: 8.422337849934896\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #validation_score (43): 8.422337849934896\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] patience losses:[8.460676574707032, 8.44919942220052, 8.435460408528646] min patience loss:8.435460408528646 current loss:8.422337849934896 absolute loss difference:0.01312255859375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] Timing: train: 0.50s, val: 0.03s, epoch: 0.52s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641805.2513912, \"EndTime\": 1731641805.77433, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 25800.0, \"count\": 1, \"min\": 25800, \"max\": 25800}, \"Total Batches Seen\": {\"sum\": 430.0, \"count\": 1, \"min\": 430, \"max\": 430}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1146.8944855162883 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:45 INFO 139684292245312] # Starting training for epoch 44\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:46.337] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 560, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] # Finished training epoch 44 on 540 examples from 9 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Subsampled 9 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) total: 8.349131661874276\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) kld: 0.06160198317633735\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) recons: 8.287529613353588\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) logppx: 8.349131661874276\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=8.349131661874276\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:46.360] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 131, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) total: 8.423741658528646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) kld: 0.056708582242329914\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) recons: 8.367032877604167\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Loss (name: value) logppx: 8.423741658528646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] #validation_score (44): 8.423741658528646\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] patience losses:[8.44919942220052, 8.435460408528646, 8.422337849934896] min patience loss:8.422337849934896 current loss:8.423741658528646 absolute loss difference:0.00140380859375\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] Timing: train: 0.56s, val: 0.02s, epoch: 0.59s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641805.774746, \"EndTime\": 1731641806.3611152, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 26400.0, \"count\": 1, \"min\": 26400, \"max\": 26400}, \"Total Batches Seen\": {\"sum\": 440.0, \"count\": 1, \"min\": 440, \"max\": 440}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1023.0230163495393 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:46 INFO 139684292245312] # Starting training for epoch 45\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:47.000] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 638, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] # Finished training epoch 45 on 600 examples from 10 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Subsampled 10 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) total: 8.342679545084636\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) kld: 0.06244100610415141\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) recons: 8.280238545735678\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) logppx: 8.342679545084636\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=8.342679545084636\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:47.024] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 134, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) total: 8.407079569498698\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) kld: 0.05370858907699585\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) recons: 8.353371175130208\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) logppx: 8.407079569498698\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #validation_score (45): 8.407079569498698\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] patience losses:[8.435460408528646, 8.422337849934896, 8.423741658528646] min patience loss:8.422337849934896 current loss:8.407079569498698 absolute loss difference:0.015258280436198035\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Timing: train: 0.64s, val: 0.03s, epoch: 0.67s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641806.3613398, \"EndTime\": 1731641807.028183, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27000.0, \"count\": 1, \"min\": 27000, \"max\": 27000}, \"Total Batches Seen\": {\"sum\": 450.0, \"count\": 1, \"min\": 450, \"max\": 450}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=899.5633337622786 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] # Starting training for epoch 46\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:47.669] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 637, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] # Finished training epoch 46 on 600 examples from 10 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Subsampled 10 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) total: 8.328514709472657\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) kld: 0.06321515043576559\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) recons: 8.265299580891927\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) logppx: 8.328514709472657\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=8.328514709472657\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:47.692] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 137, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) total: 8.39909159342448\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) kld: 0.05696969429651896\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) recons: 8.342121887207032\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Loss (name: value) logppx: 8.39909159342448\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #validation_score (46): 8.39909159342448\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] patience losses:[8.422337849934896, 8.423741658528646, 8.407079569498698] min patience loss:8.407079569498698 current loss:8.39909159342448 absolute loss difference:0.00798797607421875\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] Timing: train: 0.64s, val: 0.03s, epoch: 0.67s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641807.0284579, \"EndTime\": 1731641807.6977296, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27600.0, \"count\": 1, \"min\": 27600, \"max\": 27600}, \"Total Batches Seen\": {\"sum\": 460.0, \"count\": 1, \"min\": 460, \"max\": 460}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=896.2178459211048 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:47 INFO 139684292245312] # Starting training for epoch 47\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:48.295] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 595, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] # Finished training epoch 47 on 540 examples from 9 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Subsampled 9 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) total: 8.330740186903212\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) kld: 0.06178554782161006\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) recons: 8.268954750343605\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) logppx: 8.330740186903212\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=8.330740186903212\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:48.318] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 140, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) total: 8.399539693196614\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) kld: 0.05868029991785685\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) recons: 8.34085947672526\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) logppx: 8.399539693196614\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #validation_score (47): 8.399539693196614\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] patience losses:[8.423741658528646, 8.407079569498698, 8.39909159342448] min patience loss:8.39909159342448 current loss:8.399539693196614 absolute loss difference:0.000448099772134114\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Timing: train: 0.60s, val: 0.02s, epoch: 0.62s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641807.6981719, \"EndTime\": 1731641808.3192477, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 28200.0, \"count\": 1, \"min\": 28200, \"max\": 28200}, \"Total Batches Seen\": {\"sum\": 470.0, \"count\": 1, \"min\": 470, \"max\": 470}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=965.817511945196 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] # Starting training for epoch 48\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:48.772] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 453, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] # Finished training epoch 48 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) total: 8.289964875720797\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) kld: 0.06750480561029343\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) recons: 8.222460065569196\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) logppx: 8.289964875720797\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=8.289964875720797\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:48.795] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 143, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) total: 8.39116973876953\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) kld: 0.05474886894226074\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) recons: 8.336420694986979\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Loss (name: value) logppx: 8.39116973876953\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #validation_score (48): 8.39116973876953\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] patience losses:[8.407079569498698, 8.39909159342448, 8.399539693196614] min patience loss:8.39909159342448 current loss:8.39116973876953 absolute loss difference:0.007921854654949101\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] Timing: train: 0.45s, val: 0.03s, epoch: 0.48s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641808.3196065, \"EndTime\": 1731641808.801471, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 28800.0, \"count\": 1, \"min\": 28800, \"max\": 28800}, \"Total Batches Seen\": {\"sum\": 480.0, \"count\": 1, \"min\": 480, \"max\": 480}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1244.6301395878834 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:48 INFO 139684292245312] # Starting training for epoch 49\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.221] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 417, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] # Finished training epoch 49 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) total: 8.296640523274739\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) kld: 0.06438747975561354\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) recons: 8.23225318060981\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) logppx: 8.296640523274739\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=8.296640523274739\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 146, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) total: 8.40146993001302\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) kld: 0.060087243715922035\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) recons: 8.341382344563803\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) logppx: 8.40146993001302\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #validation_score (49): 8.40146993001302\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] patience losses:[8.39909159342448, 8.399539693196614, 8.39116973876953] min patience loss:8.39116973876953 current loss:8.40146993001302 absolute loss difference:0.010300191243489465\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Timing: train: 0.42s, val: 0.02s, epoch: 0.44s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641808.8018794, \"EndTime\": 1731641809.244889, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 29400.0, \"count\": 1, \"min\": 29400, \"max\": 29400}, \"Total Batches Seen\": {\"sum\": 490.0, \"count\": 1, \"min\": 490, \"max\": 490}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1353.9614382457237 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] \u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] # Starting training for epoch 50\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.636] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 390, \"num_examples\": 10, \"num_bytes\": 2440284}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] # Finished training epoch 50 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Metrics for Training:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) total: 8.28689456515842\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) kld: 0.07016280227237277\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) recons: 8.216731770833333\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) logppx: 8.28689456515842\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=8.28689456515842\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.657] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 149, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) total: 8.38537343343099\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) kld: 0.05706271727879842\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) recons: 8.32831064860026\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) logppx: 8.38537343343099\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #validation_score (50): 8.38537343343099\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] patience losses:[8.399539693196614, 8.39116973876953, 8.40146993001302] min patience loss:8.39116973876953 current loss:8.38537343343099 absolute loss difference:0.005796305338540364\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Timing: train: 0.39s, val: 0.03s, epoch: 0.42s\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641809.2452273, \"EndTime\": 1731641809.6629832, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 30000.0, \"count\": 1, \"min\": 30000, \"max\": 30000}, \"Total Batches Seen\": {\"sum\": 500.0, \"count\": 1, \"min\": 500, \"max\": 500}, \"Max Records Seen Between Resets\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #throughput_metric: host=algo-1, train throughput=1435.5279392563107 records/second\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 WARNING 139684292245312] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 152, \"duration\": 32, \"num_examples\": 1, \"num_bytes\": 244540}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) total: 8.488691711425782\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) kld: 0.05986728668212891\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) recons: 8.428824361165365\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) logppx: 8.488691711425782\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #quality_metric: host=algo-1, epoch=50, validation total_loss <loss>=8.488691711425782\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss of server-side model: 8.488691711425782\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Best model based on early stopping at epoch 50. Best loss: 8.38537343343099\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Topics from epoch:final (num_topics:20) [wetc 0.41, tu 0.75]:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.34, 0.84] species genus fungus stem spores mushroom fungi spore molecular cap fruit century feathers specimens distinct bird paintings maturity smooth cathedral\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.44, 0.65] game comedy episode film rating sequel home stories beyoncÃ© scenes season movie award films house don games actress role visual\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.43, 0.79] battalion submarine army church brigade forces raaf convoy fleet operation 1st government destroyer battalions guns appointed football regiment french men\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.46, 0.65] episode album girl band singles video carey audience videos live said episodes performance tour comedy house industry songs aired things\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.48, 0.73] episode film love billboard reviews rated writing drama albums don music storyline actress hollywood critics label viewers women musician films\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.42, 0.68] rock album episode song songs personnel billboard listing industry uk tracks chart lyrics production single rated liner scene veronica vocalist\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.47, 0.85] league baseball football match finals party goals victory battalion yards team victories squadron winning wins fifa training club prix appearances\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.47, 0.85] cyclone wind hurricane winds million tropical storm caused category intensity damage trees impact developed production mexico island rainfall hours warning\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.39, 0.82] torpedo guns battalion troops navy sunk knots turrets war caliber service torpedoes reserve military horsepower ship kw destroyer infantry ships\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.38, 0.78] storm hurricane mph shear flooding winds utc tropical convection landfall depression veracruz highway ny downgraded atlantic cyclone season attained pressure\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.44, 0.65] film games battalion league army team player ordered infantry troops guns game australian gun artillery ship corps naval men operations\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.33, 0.82] description genus species ground spores ecology specimens round mushroom brown Âµm phylogenetic yellow forms genera taxonomy fruit subspecies european taxa\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.51, 0.70] game race film new season music ranked live released year release song michael studio television series songs tournament years broadcast\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.24, 0.91] simpsons episode guide logan trip halo bart mood weinstein underwent staff portrayal poem claws ratings sees episodes giving ryan decides\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.32, 0.78] state road poem church city makes species entire proteins continues protein school turns daily film cells museum stories department parvati\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.39, 0.62] terminus sr highway stone intersects road routing songs number interchange route traffic chart recorded state highways hot michigan composition county\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.37, 0.73] highway route ny intersection scored road routing rerouted connector state average northern touchdown roadway measure association record crosses round sr\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.47, 0.82] episode film series tennyson fox script best plot work tells asks filming novel aired scenes reviews computer actor character book\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.45, 0.72] single number recorded reached music 100 peaked band mm track guns 000 radio performed stone rock peak armament live features\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] [0.39, 0.59] song album singles hot intersection yard track charts highway 2008 video 2003 play football piano released intersects state route yards\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Saved checkpoint to \"/tmp/tmpmuaeki1i/state-0001.params\"\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.804] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 37096, \"num_examples\": 1, \"num_bytes\": 253828}\u001b[0m\n",
            "\u001b[34m[2024-11-15 03:36:49.825] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 253828}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Metrics for Inference:\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) total: 8.386938985188802\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) kld: 0.06054646571477254\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) recons: 8.326392618815104\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] Loss (name: value) logppx: 8.386938985188802\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641809.8041415, \"EndTime\": 1731641809.8256667, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
            "\u001b[34m[11/15/2024 03:36:49 INFO 139684292245312] #test_score (algo-1) : ('log_perplexity', 8.386938985188802)\u001b[0m\n",
            "\u001b[34m#metrics {\"StartTime\": 1731641772.7020884, \"EndTime\": 1731641809.8272545, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 10636.279106140137, \"count\": 1, \"min\": 10636.279106140137, \"max\": 10636.279106140137}, \"epochs\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"model.score.time\": {\"sum\": 1234.33518409729, \"count\": 52, \"min\": 21.067380905151367, \"max\": 42.948246002197266}, \"early_stop.time\": {\"sum\": 1315.2577877044678, \"count\": 50, \"min\": 21.33321762084961, \"max\": 45.406341552734375}, \"update.time\": {\"sum\": 26287.739038467407, \"count\": 50, \"min\": 316.7920112609863, \"max\": 824.5949745178223}, \"finalize.time\": {\"sum\": 124.63116645812988, \"count\": 1, \"min\": 124.63116645812988, \"max\": 124.63116645812988}, \"model.serialize.time\": {\"sum\": 15.710592269897461, \"count\": 1, \"min\": 15.710592269897461, \"max\": 15.710592269897461}, \"setuptime\": {\"sum\": 35.60638427734375, \"count\": 1, \"min\": 35.60638427734375, \"max\": 35.60638427734375}, \"totaltime\": {\"sum\": 37179.95500564575, \"count\": 1, \"min\": 37179.95500564575, \"max\": 37179.95500564575}}}\u001b[0m\n",
            "\n",
            "2024-11-15 03:37:04 Uploading - Uploading generated training model\n",
            "2024-11-15 03:37:04 Completed - Training job completed\n",
            "Training seconds: 290\n",
            "Billable seconds: 290\n"
          ]
        }
      ],
      "source": [
        "ntm.fit({\n",
        "    'train': s3_train,\n",
        "    'validation': s3_val,\n",
        "    'auxiliary': s3_aux,\n",
        "    'test': s3_test,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35b0f03-3672-4bcb-be3d-e853e0118782",
      "metadata": {
        "tags": [],
        "id": "d35b0f03-3672-4bcb-be3d-e853e0118782",
        "outputId": "fcd46f65-9ecd-43b9-c89c-1c8b9e578b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training job name: ntm-2024-11-15-03-31-32-200\n"
          ]
        }
      ],
      "source": [
        "print(\"Training job name: {}\".format(ntm.latest_training_job.job_name))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}